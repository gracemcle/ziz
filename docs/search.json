[
  {
    "objectID": "single_cap.html",
    "href": "single_cap.html",
    "title": "Ziz Data",
    "section": "",
    "text": "The script loads metadata from a JSON file containing recording session information\nIt constructs file paths to audio chunks based on metadata\nThe first audio chunk is loaded as a NumPy array with shape (number of samples, number of channels)\nAnalysis focuses on the first 20 seconds of audio data (as specified in metadata)\n\n\n\n\n\n\n\nDisplays the raw amplitude over time for the first channel\nProvides basic temporal characteristics of the audio signal\nHelps identify regions of silence, speech, or other acoustic events\n\n\n\n\n\nComputes the frequency distribution of signal power using Welch’s method\nUses a 4096-sample window for good frequency resolution\nReveals dominant frequencies in the recording (e.g., drone motor frequencies)\nLogarithmic scale helps visualize wide range of power values\n\n\n\n\n\nCreates a time-frequency representation showing how spectral content changes over time\nUses 1024-sample windows with overlap\nConverts magnitude to decibels for better visibility\nColor intensity indicates energy level at each time-frequency point\n\n\n\n\n\nCalculates RMS (Root Mean Square) energy in 100ms windows for all channels\nPlots energy levels over time for each channel\nEnables comparison of signal strength across the microphone array\nHelpful for identifying directional characteristics of sound sources\n\n\n\n\n\nCreates a three-dimensional visualization showing time-frequency content for multiple channels\nSelects a subset of channels (up to 5) for clarity\nDownsamples the data for better visualization performance\nOffsets each channel’s surface plot for clear separation\nProvides spatial-spectral overview of the recording\n\n\n\n\n\nFor 16-channel UMA-16V2 array recordings only\nCreates a polar plot showing average energy distribution across channels\nAssumes microphones are arranged in a circle\nVisualizes the directional characteristics of the sound field\nHelps identify direction of arrival for sound sources\n\n\n\n\n\nThis visualization pipeline is particularly valuable for: - Analyzing drone acoustic signatures - Spatial audio research - Sound source localization - Microphone array performance evaluation - Environmental sound monitoring\n\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import welch, spectrogram\nfrom scipy import signal\nimport pandas as pd\nimport librosa\nimport librosa.display\n\n# Configuration\ncfg = {\n    'metadata_local_path_str': '/home/grace/Develop/ziz/2025-04-21_19-08-01_Session1_MJ_Brown Building Apartments_Sunny/metadata.json'\n}\n\nif __name__ == '__main__':\n    # Open and read the metadata file\n    metadata_json = Path(cfg['metadata_local_path_str'])\n    local_path = metadata_json.parent\n    \n    with metadata_json.open('r') as f:\n        metadata = json.load(f)\n    \n    # Collect audio chunk paths\n    audio_chunks_over_time = []\n    for audio_metadata in metadata['audio_chunks_timestamps']:\n        orig_path = Path(audio_metadata['file_name'])\n        last_two = orig_path.parts[-2:]\n        audio_path = local_path.joinpath(*last_two)\n        audio_chunks_over_time.append(audio_path)\n    \n    # Load the first audio chunk\n    idx = 0\n    audio_data_np = np.load(audio_chunks_over_time[idx])  # shape: (n_samples, n_channels)\n    expected_sample_rate = int(metadata['sound_card_sample_rate'])\n    print(f'Sample rate: {expected_sample_rate} Hz')\n    print(f'Audio data shape: {audio_data_np.shape}')\n    \n    # Use first 20 seconds (according to chunk size in metadata)\n    duration_seconds = metadata['chunk_size_in_seconds']\n    num_samples = min(int(expected_sample_rate * duration_seconds), audio_data_np.shape[0])\n    print(f'Analyzing {duration_seconds} seconds ({num_samples} samples)')\n    \n    # Create a figure with multiple subplots\n    plt.figure(figsize=(15, 20))\n    \n    # ===== PLOT 1: Waveform of first channel =====\n    plt.subplot(5, 1, 1)\n    time = np.arange(num_samples) / expected_sample_rate\n    channel = 0  # First channel\n    plt.plot(time, audio_data_np[:num_samples, channel])\n    plt.title(f'Waveform (Channel {channel})')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n    \n    # ===== PLOT 2: Power Spectral Density =====\n    plt.subplot(5, 1, 2)\n    freqs, power = welch(audio_data_np[:num_samples, channel], fs=expected_sample_rate, nperseg=4096)\n    plt.semilogy(freqs, power)\n    plt.title(f'Power Spectral Density (Channel {channel})')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Power Spectral Density (V²/Hz)')\n    plt.grid(True)\n    \n    # ===== PLOT 3: Spectrogram =====\n    plt.subplot(5, 1, 3)\n    f, t, Sxx = spectrogram(audio_data_np[:num_samples, channel], fs=expected_sample_rate, nperseg=1024)\n    plt.pcolormesh(t, f, 10 * np.log10(Sxx), shading='gouraud')\n    plt.title(f'Spectrogram (Channel {channel})')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Frequency (Hz)')\n    plt.colorbar(label='Intensity (dB)')\n    \n    # ===== PLOT 4: Channel Comparison (RMS Energy) =====\n    plt.subplot(5, 1, 4)\n    \n    # Calculate RMS energy for each channel\n    n_channels = min(16, audio_data_np.shape[1])  # Use up to 16 channels\n    channel_energies = []\n    \n    for ch in range(n_channels):\n        # Calculate RMS energy in 100ms windows\n        window_size = int(expected_sample_rate * 0.1)\n        n_windows = num_samples // window_size\n        energy = []\n        \n        for i in range(n_windows):\n            start = i * window_size\n            end = (i + 1) * window_size\n            segment = audio_data_np[start:end, ch]\n            rms = np.sqrt(np.mean(np.square(segment)))\n            energy.append(rms)\n        \n        channel_energies.append(energy)\n    \n    # Create time axis for energy plot\n    energy_time = np.arange(len(channel_energies[0])) * 0.1  # 100ms windows\n    \n    # Plot energies for each channel\n    for ch in range(n_channels):\n        plt.plot(energy_time, channel_energies[ch], label=f'Channel {ch}')\n    \n    plt.title('Channel Energy Comparison (RMS)')\n    plt.xlabel('Time (s)')\n    plt.ylabel('RMS Energy')\n    plt.legend(loc='upper right', fontsize='small')\n    plt.grid(True)\n    \n    # ===== PLOT 5: 3D Visualization (Multiple channels over time and frequency) =====\n    plt.subplot(5, 1, 5, projection='3d')\n    \n    # Select a subset of channels for clarity\n    channels_to_plot = min(5, n_channels)\n    channel_indices = np.linspace(0, n_channels-1, channels_to_plot, dtype=int)\n    \n    for i, ch in enumerate(channel_indices):\n        # Calculate spectrogram\n        f, t, Sxx = spectrogram(audio_data_np[:num_samples, ch], fs=expected_sample_rate, nperseg=1024)\n        \n        # Downsample for visualization\n        freq_step = max(1, len(f) // 50)\n        time_step = max(1, len(t) // 50)\n        \n        f_downsample = f[::freq_step]\n        t_downsample = t[::time_step]\n        Sxx_downsample = Sxx[::freq_step, ::time_step]\n        \n        # Convert to dB for better visualization\n        Sxx_db = 10 * np.log10(Sxx_downsample + 1e-10)\n        \n        # Create meshgrid for 3D plot\n        T, F = np.meshgrid(t_downsample, f_downsample)\n        \n        # Plot as surface with offset based on channel number\n        offset = i * 20  # Offset each channel for visibility\n        ax = plt.gca()\n        surf = ax.plot_surface(T, F, Sxx_db + offset, cmap='viridis', alpha=0.7)\n    \n    plt.title('3D Spectrogram Comparison (Multiple Channels)')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Frequency (Hz)')\n    \n    # Add custom legend for channels\n    from matplotlib.lines import Line2D\n    legend_elements = []\n    for i, ch in enumerate(channel_indices):\n        legend_elements.append(Line2D([0], [0], color=plt.cm.viridis(i/channels_to_plot), lw=4, label=f'Channel {ch}'))\n    ax.legend(handles=legend_elements, loc='upper right')\n    \n    plt.tight_layout()\n    plt.savefig('audio_visualization.png', dpi=300)\n    plt.show()\n    \n    \n    # Spatial audio visualization (if this is a 16-channel UMA-16V2 array)\n    if n_channels &gt;= 16 and metadata['audio_sensor_name'] == 'UMA-16V2 USB Mic Array':\n        plt.figure(figsize=(10, 10))\n        \n        # Assume a circular microphone array with 16 channels\n        # Plot the energy distribution in a polar coordinate system\n        \n        # Calculate average energy for each channel over time\n        avg_energy = [np.mean(energy) for energy in channel_energies]\n        \n        # Normalize for visualization\n        normalized_energy = np.array(avg_energy) / max(avg_energy)\n        \n        # Create angles for the circular array (UMA-16V2 has 16 mics arranged in a circle)\n        angles = np.linspace(0, 2*np.pi, n_channels, endpoint=False)\n        \n        # Plot in polar coordinates\n        ax = plt.subplot(111, polar=True)\n        ax.fill(angles, normalized_energy, alpha=0.25)\n        ax.plot(angles, normalized_energy, 'o-', linewidth=2)\n        \n        # Make the plot circular by connecting back to the first point\n        angles_closed = np.append(angles, angles[0])\n        energy_closed = np.append(normalized_energy, normalized_energy[0])\n        ax.plot(angles_closed, energy_closed, 'o-', linewidth=2)\n        \n        # Add channel numbers\n        for i in range(n_channels):\n            ax.text(angles[i], normalized_energy[i] + 0.1, f'{i}', \n                   horizontalalignment='center', verticalalignment='center')\n        \n        plt.title('Spatial Energy Distribution (UMA-16V2 Array)')\n        plt.tight_layout()\n        plt.savefig('spatial_distribution.png', dpi=300)\n        plt.show()\n\nSample rate: 44100 Hz\nAudio data shape: (882000, 16)\nAnalyzing 20 seconds (882000 samples)"
  },
  {
    "objectID": "single_cap.html#data-loading-and-preparation",
    "href": "single_cap.html#data-loading-and-preparation",
    "title": "Ziz Data",
    "section": "",
    "text": "The script loads metadata from a JSON file containing recording session information\nIt constructs file paths to audio chunks based on metadata\nThe first audio chunk is loaded as a NumPy array with shape (number of samples, number of channels)\nAnalysis focuses on the first 20 seconds of audio data (as specified in metadata)"
  },
  {
    "objectID": "single_cap.html#visualization-types",
    "href": "single_cap.html#visualization-types",
    "title": "Ziz Data",
    "section": "",
    "text": "Displays the raw amplitude over time for the first channel\nProvides basic temporal characteristics of the audio signal\nHelps identify regions of silence, speech, or other acoustic events\n\n\n\n\n\nComputes the frequency distribution of signal power using Welch’s method\nUses a 4096-sample window for good frequency resolution\nReveals dominant frequencies in the recording (e.g., drone motor frequencies)\nLogarithmic scale helps visualize wide range of power values\n\n\n\n\n\nCreates a time-frequency representation showing how spectral content changes over time\nUses 1024-sample windows with overlap\nConverts magnitude to decibels for better visibility\nColor intensity indicates energy level at each time-frequency point\n\n\n\n\n\nCalculates RMS (Root Mean Square) energy in 100ms windows for all channels\nPlots energy levels over time for each channel\nEnables comparison of signal strength across the microphone array\nHelpful for identifying directional characteristics of sound sources\n\n\n\n\n\nCreates a three-dimensional visualization showing time-frequency content for multiple channels\nSelects a subset of channels (up to 5) for clarity\nDownsamples the data for better visualization performance\nOffsets each channel’s surface plot for clear separation\nProvides spatial-spectral overview of the recording\n\n\n\n\n\nFor 16-channel UMA-16V2 array recordings only\nCreates a polar plot showing average energy distribution across channels\nAssumes microphones are arranged in a circle\nVisualizes the directional characteristics of the sound field\nHelps identify direction of arrival for sound sources"
  },
  {
    "objectID": "single_cap.html#applications",
    "href": "single_cap.html#applications",
    "title": "Ziz Data",
    "section": "",
    "text": "This visualization pipeline is particularly valuable for: - Analyzing drone acoustic signatures - Spatial audio research - Sound source localization - Microphone array performance evaluation - Environmental sound monitoring\n\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import welch, spectrogram\nfrom scipy import signal\nimport pandas as pd\nimport librosa\nimport librosa.display\n\n# Configuration\ncfg = {\n    'metadata_local_path_str': '/home/grace/Develop/ziz/2025-04-21_19-08-01_Session1_MJ_Brown Building Apartments_Sunny/metadata.json'\n}\n\nif __name__ == '__main__':\n    # Open and read the metadata file\n    metadata_json = Path(cfg['metadata_local_path_str'])\n    local_path = metadata_json.parent\n    \n    with metadata_json.open('r') as f:\n        metadata = json.load(f)\n    \n    # Collect audio chunk paths\n    audio_chunks_over_time = []\n    for audio_metadata in metadata['audio_chunks_timestamps']:\n        orig_path = Path(audio_metadata['file_name'])\n        last_two = orig_path.parts[-2:]\n        audio_path = local_path.joinpath(*last_two)\n        audio_chunks_over_time.append(audio_path)\n    \n    # Load the first audio chunk\n    idx = 0\n    audio_data_np = np.load(audio_chunks_over_time[idx])  # shape: (n_samples, n_channels)\n    expected_sample_rate = int(metadata['sound_card_sample_rate'])\n    print(f'Sample rate: {expected_sample_rate} Hz')\n    print(f'Audio data shape: {audio_data_np.shape}')\n    \n    # Use first 20 seconds (according to chunk size in metadata)\n    duration_seconds = metadata['chunk_size_in_seconds']\n    num_samples = min(int(expected_sample_rate * duration_seconds), audio_data_np.shape[0])\n    print(f'Analyzing {duration_seconds} seconds ({num_samples} samples)')\n    \n    # Create a figure with multiple subplots\n    plt.figure(figsize=(15, 20))\n    \n    # ===== PLOT 1: Waveform of first channel =====\n    plt.subplot(5, 1, 1)\n    time = np.arange(num_samples) / expected_sample_rate\n    channel = 0  # First channel\n    plt.plot(time, audio_data_np[:num_samples, channel])\n    plt.title(f'Waveform (Channel {channel})')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n    \n    # ===== PLOT 2: Power Spectral Density =====\n    plt.subplot(5, 1, 2)\n    freqs, power = welch(audio_data_np[:num_samples, channel], fs=expected_sample_rate, nperseg=4096)\n    plt.semilogy(freqs, power)\n    plt.title(f'Power Spectral Density (Channel {channel})')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Power Spectral Density (V²/Hz)')\n    plt.grid(True)\n    \n    # ===== PLOT 3: Spectrogram =====\n    plt.subplot(5, 1, 3)\n    f, t, Sxx = spectrogram(audio_data_np[:num_samples, channel], fs=expected_sample_rate, nperseg=1024)\n    plt.pcolormesh(t, f, 10 * np.log10(Sxx), shading='gouraud')\n    plt.title(f'Spectrogram (Channel {channel})')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Frequency (Hz)')\n    plt.colorbar(label='Intensity (dB)')\n    \n    # ===== PLOT 4: Channel Comparison (RMS Energy) =====\n    plt.subplot(5, 1, 4)\n    \n    # Calculate RMS energy for each channel\n    n_channels = min(16, audio_data_np.shape[1])  # Use up to 16 channels\n    channel_energies = []\n    \n    for ch in range(n_channels):\n        # Calculate RMS energy in 100ms windows\n        window_size = int(expected_sample_rate * 0.1)\n        n_windows = num_samples // window_size\n        energy = []\n        \n        for i in range(n_windows):\n            start = i * window_size\n            end = (i + 1) * window_size\n            segment = audio_data_np[start:end, ch]\n            rms = np.sqrt(np.mean(np.square(segment)))\n            energy.append(rms)\n        \n        channel_energies.append(energy)\n    \n    # Create time axis for energy plot\n    energy_time = np.arange(len(channel_energies[0])) * 0.1  # 100ms windows\n    \n    # Plot energies for each channel\n    for ch in range(n_channels):\n        plt.plot(energy_time, channel_energies[ch], label=f'Channel {ch}')\n    \n    plt.title('Channel Energy Comparison (RMS)')\n    plt.xlabel('Time (s)')\n    plt.ylabel('RMS Energy')\n    plt.legend(loc='upper right', fontsize='small')\n    plt.grid(True)\n    \n    # ===== PLOT 5: 3D Visualization (Multiple channels over time and frequency) =====\n    plt.subplot(5, 1, 5, projection='3d')\n    \n    # Select a subset of channels for clarity\n    channels_to_plot = min(5, n_channels)\n    channel_indices = np.linspace(0, n_channels-1, channels_to_plot, dtype=int)\n    \n    for i, ch in enumerate(channel_indices):\n        # Calculate spectrogram\n        f, t, Sxx = spectrogram(audio_data_np[:num_samples, ch], fs=expected_sample_rate, nperseg=1024)\n        \n        # Downsample for visualization\n        freq_step = max(1, len(f) // 50)\n        time_step = max(1, len(t) // 50)\n        \n        f_downsample = f[::freq_step]\n        t_downsample = t[::time_step]\n        Sxx_downsample = Sxx[::freq_step, ::time_step]\n        \n        # Convert to dB for better visualization\n        Sxx_db = 10 * np.log10(Sxx_downsample + 1e-10)\n        \n        # Create meshgrid for 3D plot\n        T, F = np.meshgrid(t_downsample, f_downsample)\n        \n        # Plot as surface with offset based on channel number\n        offset = i * 20  # Offset each channel for visibility\n        ax = plt.gca()\n        surf = ax.plot_surface(T, F, Sxx_db + offset, cmap='viridis', alpha=0.7)\n    \n    plt.title('3D Spectrogram Comparison (Multiple Channels)')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Frequency (Hz)')\n    \n    # Add custom legend for channels\n    from matplotlib.lines import Line2D\n    legend_elements = []\n    for i, ch in enumerate(channel_indices):\n        legend_elements.append(Line2D([0], [0], color=plt.cm.viridis(i/channels_to_plot), lw=4, label=f'Channel {ch}'))\n    ax.legend(handles=legend_elements, loc='upper right')\n    \n    plt.tight_layout()\n    plt.savefig('audio_visualization.png', dpi=300)\n    plt.show()\n    \n    \n    # Spatial audio visualization (if this is a 16-channel UMA-16V2 array)\n    if n_channels &gt;= 16 and metadata['audio_sensor_name'] == 'UMA-16V2 USB Mic Array':\n        plt.figure(figsize=(10, 10))\n        \n        # Assume a circular microphone array with 16 channels\n        # Plot the energy distribution in a polar coordinate system\n        \n        # Calculate average energy for each channel over time\n        avg_energy = [np.mean(energy) for energy in channel_energies]\n        \n        # Normalize for visualization\n        normalized_energy = np.array(avg_energy) / max(avg_energy)\n        \n        # Create angles for the circular array (UMA-16V2 has 16 mics arranged in a circle)\n        angles = np.linspace(0, 2*np.pi, n_channels, endpoint=False)\n        \n        # Plot in polar coordinates\n        ax = plt.subplot(111, polar=True)\n        ax.fill(angles, normalized_energy, alpha=0.25)\n        ax.plot(angles, normalized_energy, 'o-', linewidth=2)\n        \n        # Make the plot circular by connecting back to the first point\n        angles_closed = np.append(angles, angles[0])\n        energy_closed = np.append(normalized_energy, normalized_energy[0])\n        ax.plot(angles_closed, energy_closed, 'o-', linewidth=2)\n        \n        # Add channel numbers\n        for i in range(n_channels):\n            ax.text(angles[i], normalized_energy[i] + 0.1, f'{i}', \n                   horizontalalignment='center', verticalalignment='center')\n        \n        plt.title('Spatial Energy Distribution (UMA-16V2 Array)')\n        plt.tight_layout()\n        plt.savefig('spatial_distribution.png', dpi=300)\n        plt.show()\n\nSample rate: 44100 Hz\nAudio data shape: (882000, 16)\nAnalyzing 20 seconds (882000 samples)"
  },
  {
    "objectID": "single_cap.html#data-preprocessing-pipeline",
    "href": "single_cap.html#data-preprocessing-pipeline",
    "title": "Ziz Data",
    "section": "Data Preprocessing Pipeline",
    "text": "Data Preprocessing Pipeline\nThe acoustic model processes audio data through several key steps before training:\n\n1. Input Format Transformation\n\nSpectrograms are used as input instead of raw audio signals\nSpectrograms provide frequency-domain representation that better captures acoustic patterns\n\n\n\n2. Audio Signal Resampling\n\nAll six microphone channels are resampled to 48kHz\nThis ensures uniform sample rate across all audio inputs\nConsistent sample rate is critical for model training stability\n\n\n\n3. Temporal Segmentation\n\nEach channel’s audio stream is divided into 1-second chunks\nThese chunks are further subdivided into 30 segments\nEach segment corresponds to a single frame from the synchronized video data\nThis creates time-aligned audio-visual data pairs\n\n\n\n4. Spectrogram Generation\n\nShort-Time Fourier Transform (STFT) is applied to each audio segment\nSTFT parameters:\n\nHamming window function for better frequency resolution\nWindow size of 512 samples\n4 hops per window (hop length = 128 samples)\n\nThis configuration yields spectrograms with dimensions [257, 376]\n\n257 frequency bins\n376 time frames\n\n\n\n\n5. Channel Stacking\n\nSpectrograms from all 6 channels for each segment are stacked together\nThis preserves spatial audio information from the microphone array\nThe resulting multi-channel spectrograms serve as input to the neural network\n\n\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nfrom scipy import signal\nimport pandas as pd\n\n# Configuration\ncfg = {\n    'metadata_local_path_str': '/home/grace/Develop/ziz/2025-04-21_19-08-01_Session1_MJ_Brown Building Apartments_Sunny/metadata.json'\n}\n\ndef plot_waveform(audio_data, sr, title, ax):\n    \"\"\"Plot a waveform of audio data.\"\"\"\n    librosa.display.waveshow(audio_data, sr=sr, ax=ax)\n    ax.set_title(title)\n    ax.set_xlabel(\"Time (s)\")\n    ax.set_ylabel(\"Amplitude\")\n\ndef plot_spectrogram(spectrogram, title, ax):\n    \"\"\"Plot a spectrogram.\"\"\"\n    librosa.display.specshow(librosa.amplitude_to_db(spectrogram, ref=np.max),\n                             y_axis='log', x_axis='time', ax=ax)\n    ax.set_title(title)\n    ax.set_xlabel(\"Time (frames)\")\n    ax.set_ylabel(\"Frequency (Hz)\")\n    plt.colorbar(ax.collections[0], ax=ax, format='%+2.0f dB')\n\nif __name__ == '__main__':\n    # Open and read the metadata file\n    metadata_json = Path(cfg['metadata_local_path_str'])\n    local_path = metadata_json.parent\n    \n    with metadata_json.open('r') as f:\n        metadata = json.load(f)\n    \n    # Collect audio chunk paths\n    audio_chunks_over_time = []\n    for audio_metadata in metadata['audio_chunks_timestamps']:\n        orig_path = Path(audio_metadata['file_name'])\n        last_two = orig_path.parts[-2:]\n        audio_path = local_path.joinpath(*last_two)\n        audio_chunks_over_time.append(audio_path)\n    \n    # Load the first audio chunk\n    idx = 0\n    audio_data_np = np.load(audio_chunks_over_time[idx])  # shape: (n_samples, n_channels)\n    original_sample_rate = int(metadata['sound_card_sample_rate'])\n    print(f'Original sample rate: {original_sample_rate} Hz')\n    print(f'Audio data shape: {audio_data_np.shape}')\n\n    # According to instructions, we should use 6 channels\n    # If there are more, we'll take the first 6\n    num_channels = min(6, audio_data_np.shape[1])\n    \n    # Create a figure for the full preprocessing pipeline\n    fig = plt.figure(figsize=(15, 25))\n    \n    # STEP 1: Original audio waveform for each channel\n    orig_axes = []\n    for ch in range(num_channels):\n        ax = plt.subplot(num_channels, 3, ch*3 + 1)\n        plot_waveform(audio_data_np[:5*original_sample_rate, ch], original_sample_rate, \n                     f\"Original Channel {ch} Waveform (First 5s)\", ax)\n        orig_axes.append(ax)\n    \n    # STEP 2: Resample to 48kHz\n    target_sample_rate = 48000\n    \n    # Don't pre-allocate with fixed size - resample each channel first, then stack\n    resampled_channels = []\n    \n    for ch in range(num_channels):\n        # Resample each channel individually\n        resampled_channel = librosa.resample(\n            audio_data_np[:, ch], \n            orig_sr=original_sample_rate, \n            target_sr=target_sample_rate\n        )\n        resampled_channels.append(resampled_channel)\n    \n    # Find the minimum length (to handle any potential length differences)\n    min_length = min(len(channel) for channel in resampled_channels)\n    \n    # Create the final array with consistent dimensions\n    resampled_audio = np.zeros((min_length, num_channels))\n    for ch in range(num_channels):\n        resampled_audio[:, ch] = resampled_channels[ch][:min_length]\n    \n    print(f'Resampled audio shape: {resampled_audio.shape}')\n    \n    # Plot resampled waveforms\n    resampled_axes = []\n    for ch in range(num_channels):\n        ax = plt.subplot(num_channels, 3, ch*3 + 2)\n        plot_waveform(resampled_audio[:5*target_sample_rate, ch], target_sample_rate, \n                     f\"Resampled Channel {ch} Waveform (First 5s)\", ax)\n        resampled_axes.append(ax)\n    \n    # STEP 3: Divide into 1s chunks and compute spectrograms\n    chunk_duration = 1  # 1 second chunks\n    samples_per_chunk = target_sample_rate * chunk_duration\n    \n    # Select the first chunk for demonstration\n    first_chunk = resampled_audio[:samples_per_chunk, :]\n    \n    # Set STFT parameters as specified\n    n_fft = 512  # Window size\n    hop_length = n_fft // 4  # 4 Hops as specified\n    \n    # Compute and display spectrograms for each channel\n    spec_axes = []\n    spectrograms = []\n    \n    for ch in range(num_channels):\n        # Apply STFT with Hamming window\n        spectrogram = librosa.stft(\n            first_chunk[:, ch], \n            n_fft=n_fft, \n            hop_length=hop_length, \n            window='hamming'\n        )\n        spectrograms.append(spectrogram)\n        \n        # Plot spectrogram\n        ax = plt.subplot(num_channels, 3, ch*3 + 3)\n        plot_spectrogram(spectrogram, f\"Spectrogram Channel {ch} (1s chunk)\", ax)\n        spec_axes.append(ax)\n    \n    plt.tight_layout()\n    # plt.savefig('preprocessing_visualization.png', dpi=300)\n    \n    # Create a new figure to show the stacked spectrograms (input to the model)\n    plt.figure(figsize=(12, 10))\n    \n    # Stack spectrograms and visualize\n    # First, convert to magnitude spectrograms\n    mag_specs = [np.abs(spec) for spec in spectrograms]\n    \n    # Check the shape\n    first_spec_shape = mag_specs[0].shape\n    print(f\"Spectrogram shape: {first_spec_shape}\")  # Should be close to [257×273]\n    \n    # Visualize the stacked spectrograms as input to the model\n    # We'll create a grid of mini-spectrograms and then show a 3D representation\n    \n    # 2D grid visualization\n    plt.subplot(2, 1, 1)\n    combined_spec = np.hstack(mag_specs)\n    librosa.display.specshow(\n        librosa.amplitude_to_db(combined_spec, ref=np.max),\n        y_axis='log', \n        x_axis='time'\n    )\n    plt.title(\"Stacked Channel Spectrograms (Input to Model)\")\n    plt.xlabel(\"Time (frames) × Channels\")\n    plt.ylabel(\"Frequency (Hz)\")\n    plt.colorbar(format='%+2.0f dB')\n    \n    # 3D visualization of the stacked spectrograms\n    ax = plt.subplot(2, 1, 2, projection='3d')\n    \n    for ch in range(num_channels):\n        # Get the spectrogram for this channel\n        spec = librosa.amplitude_to_db(mag_specs[ch], ref=np.max)\n        \n        # Downsample for visualization clarity\n        freq_step = max(1, spec.shape[0] // 50)\n        time_step = max(1, spec.shape[1] // 50)\n        spec_downsampled = spec[::freq_step, ::time_step]\n        \n        # Create a meshgrid\n        x, y = np.meshgrid(\n            np.arange(0, spec_downsampled.shape[1]),\n            np.arange(0, spec_downsampled.shape[0])\n        )\n        \n        # Add channel offset for 3D visualization\n        x = x + ch * (spec_downsampled.shape[1] + 5)\n        \n        # Plot as a surface\n        surf = ax.plot_surface(\n            x, y, spec_downsampled,\n            cmap=plt.cm.viridis,\n            alpha=0.7\n        )\n    \n    ax.set_title(\"3D View of Stacked Spectrograms\")\n    ax.set_xlabel(\"Time (frames) × Channels\")\n    ax.set_ylabel(\"Frequency Bins\")\n    ax.set_zlabel(\"Magnitude (dB)\")\n    \n    plt.tight_layout()\n    # plt.savefig('stacked_spectrograms_input.png', dpi=300)\n\nOriginal sample rate: 44100 Hz\nAudio data shape: (882000, 16)\nResampled audio shape: (960001, 6)\n\n\n/tmp/ipykernel_2077780/2994945382.py:24: UserWarning: amplitude_to_db was called on complex input so phase information will be discarded. To suppress this warning, call amplitude_to_db(np.abs(S)) instead.\n  librosa.display.specshow(librosa.amplitude_to_db(spectrogram, ref=np.max),\n\n\nSpectrogram shape: (257, 376)"
  }
]